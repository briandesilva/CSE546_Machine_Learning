\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\usepackage{subfig}
\usepackage{enumerate}          % For enumerates indexed by letters
\usepackage{bm}                 % For bold letters
\usepackage{algorithm2e}        % For pseudocode

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass:\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\setcounter{section}{-1}




%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework 3}
\newcommand{\hmwkDueDate}{November ***, 2016}
\newcommand{\hmwkClass}{CSE 546}
\newcommand{\hmwkAuthorName}{Brian de Silva}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ }\\
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}


% Useful commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\y}{\hat \bm{y}}
\newcommand{\yi}{\hat \bm{y_i}}
\newcommand{\X}{\bm{X}}
\newcommand{\w}{\bm{w}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Tr}{\mathrm{Tr}}


\begin{document}

\maketitle

\pagebreak

% Problem 0
\section{Collaborators and Acknowledgements}


% Problem 1
\section{PCA and reconstruction}
\subsection{Matrix Algebra Review}
\begin{enumerate}
	\item Recall that for $A\in\R^{n\times d}$ and $C\in\R^{d\times n}$, $(AC)_{ij} = \sum^d_{k=1}a_{ik}c_{kj}$. Also, $(B^T)_{ij} = B_{ji}$. Hence
	\[
		(AB^T)_{ij} = \sum^d_{i=1}(A)_{ik}(B^T)_{jk} = \sum^d_{i=1}a_{ik}b_{kj}.
	\]

	Plugging this into the definition of the trace gives
	\begin{align*}
		\Tr(AB^T) &= \sum^n_{i=1}\left(\sum^d_{k=1}(AB^T)_{ii}\right)\\
		&= \sum^n_{i=1}\left(\sum^d_{k=1}a_{ik}b_{ik}\right).
	\end{align*}
	Similarly,
	\[
		(B^TA)_{ij} = \sum^{n}_{k=1}b_{ki}a_{kj}
	\]
	and, by switching the order of addition,
	\begin{align*}
		\Tr(B^TA) &= \sum^d_{i=1}\left(B^TA\right)_{ii}\\
		&= \sum^d_{i=1}\left(\sum^n_{k=1}b_{ki}a_{ki} \right) \\
		&= \sum^n_{i=1}\left(\sum^d_{k=1}b_{ki}a_{ki} \right) \\
		&= \Tr(AB^T).
	\end{align*}

	\item The outer equality follows from the definition of the trace:
	\begin{align*}
		\Tr(\Sigma) = \Tr\left(\tfrac1nX^TX\right) = \frac1n\sum^d_{i=1}(X^TX)_{ii} &= \frac1n \sum^d_{i=1}\left( \sum^n_{k=1}x_{ik}^2 \right)\\
		&= \frac1n\sum^d_{i=1}\|X_i\|^2.
	\end{align*}
	For the other equality, we need some standard linear algebra results. Since $\Sigma$ is symmetric and real, it has a real orthogonal eigendecomposition. That is to say, there exists an orthogonal matrix $Q$ and a diagonal matrix $\Lambda$ with diagonal entries $\lambda_1,\lambda_2,\dots,\lambda_d$ such that
	\[
		\Sigma = Q\Lambda Q^T.
	\]
	Using our result from part 1, we have
	\[
		\Tr(\Sigma) = \Tr\left(Q\Lambda Q^T\right) = \Tr\left((Q\Lambda) Q^T\right) = \Tr\left(Q^TQ\Lambda\right) = \Tr(\Lambda) = \sum^d_{i=1}\lambda_i.
	\]
\end{enumerate}

\subsection{PCA}
\begin{enumerate}
	\item Without centering the data, the eigenvalues we obtain for $\Sigma$ are 
	\begin{align*}
		\lambda_1 &= 2476871.877922\\
		\lambda_2 &= 285468.253236\\
		\lambda_{10} &= 81022.153964\\
		\lambda_{30} &= 23690.383772\\
		\lambda_{50} &= 11076.827129.
	\end{align*}
	The sum of the eigenvalues is $\sum^{784}_{i=1}\lambda_i = 5709846.669117$. Notice that the first eigenvalue accounts for almost half of the total of all the eigenvalues.
	\item
	\begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{fre1}
        \caption{Fractional reconstruction error using the top 1 through 50 principal components} 
        \label{fig:fre1}
    \end{figure}
	We plot the fractional reconstruction error retaining 1 through 50 of the top principal directions in Figure \ref{fig:fre1}.
	\item The first eigenvector captures the average image of the dataset. The first eigenvalue gives some indication of how close the images are to this average one. All of the images have a large amount of blank space in common, and so they are, in a sense, well-approximated by the average image. This explains why the first eigenvalue is so large compared to the rest.
	\item 
	\begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{fre2}
        \caption{Fractional reconstruction error using the top 2 through 50 principal components} 
        \label{fig:fre2}
    \end{figure}
    We show the fractional reconstruction error for dimensions 2 through 50 in Figure \ref{fig:fre2}.
\end{enumerate}

\subsection{Visualization of the Eigen-Directions}
\begin{enumerate}
	\item
	\begin{figure}
        \centering
        \includegraphics[width=.95\textwidth]{topEvs}
        \caption{The eigenvectors corresponding to the 10 largest eigenvalues of $\Sigma$. Grey corresponds to values of 0, black to positive values, and white to negative values.} 
        \label{fig:topEvs}
    \end{figure}
	Visualizations of the top 10 eigenvectors are shown in Figure \ref{fig:topEvs}.
	\item As I mentioned in a previous problem, the first eigenvector captures the average of all the images. The other eigenvalues appear to contain information about patterns encountered in many of the digits. The second eigenvalue, for example, appears to encapsulate the roundness that many digits exhibit (i.e. 0,3,6,8, and 9). 
\end{enumerate}

\subsection{Visualization and Reconstruction}
\begin{enumerate}
	\item 
	\begin{figure}
        \centering
        \includegraphics[width=.95\textwidth]{digits}
        \caption{Five different images from the MNIST dataset} 
        \label{fig:digits}
    \end{figure}
    Figure \ref{fig:digits} shows five images from the MNIST dataset (the first five, in fact).
    \item 
    \begin{figure}
        \centering
        \begin{subfigure}[b]{0.95\textwidth}
        	\centering
        	\includegraphics[width=0.7\linewidth]{02evRecon}
        	\caption{Reconstruction with two eigenvectors}
        	\label{fig:2ev}
        \end{subfigure}
        \begin{subfigure}[b]{0.95\textwidth}
        	\centering
        	\includegraphics[width=0.7\linewidth]{05evRecon}
        	\caption{Reconstruction with five eigenvectors}
        	\label{fig:4ev}
        \end{subfigure}
        \caption{Reconstruction of MNIST images using two and five eigenvectors}
        \label{fig:evRecons1}
    \end{figure}

    \begin{figure}
    	\centering
        \begin{subfigure}[b]{0.95\textwidth}
        	\centering
        	\includegraphics[width=0.7\linewidth]{10evRecon}
        	\caption{Reconstruction with 10 eigenvectors}
        	\label{fig:10ev}
        \end{subfigure}
        \begin{subfigure}[b]{0.95\textwidth}
        	\centering
        	\includegraphics[width=0.7\linewidth]{20evRecon}
        	\caption{Reconstruction with 20 eigenvectors}
        	\label{fig:20ev}
        \end{subfigure}
        \begin{subfigure}[b]{0.95\textwidth}
        	\centering
        	\includegraphics[width=0.7\linewidth]{50evRecon}
        	\caption{Reconstruction with 50 eigenvectors}
        	\label{fig:50ev}
        \end{subfigure}
        \caption{Reconstructions of MNIST images using different numbers of eigenvectors} 
        \label{fig:evRecons2}
    \end{figure}
    Figure \ref{fig:evRecons1} shows the reconstructions produced using two and five eigenvectors, respectively. Figure \ref{fig:evRecons2} visualizes the reconstructions utilizing 10, 20, and 50 eigenvectors, respectively.
    \item The two-eigenvector reconstruction is not very good. Under this projection some of the digits look almost identical. Using just five eigenvectors, one can already make out most of the digits. With 10 eigenvectors these digits become even clearer, though the 4 appears to be difficult to represent. The 20-eigenvalue approximation is crisper than the previous ones, but the increase in quality it starting to slow. Adding another 30 eigenvalues makes it very easy to discern the digits. Overall, adding more eigenvalues improves the projection quality, but the returns are diminishing.

\end{enumerate}


% Problem 3
\section{SVMs: Hinge loss and mistake bounds}
\begin{enumerate}
	\item To show that $\ell((x,y),w)=\max\{0,1-w\cdot x\}$ is convex with respect to $w$, we will need two observations. First, for any $a,k\in\R$ with $k\geq 0$, we have that
	\[
		\max\{0,ka\} = k\max\{0,a\}.
	\]
	In the case $ka<0$, both sides of the equality are 0. If $ka\geq 0$, then $a\geq0$ and the equality still holds. Next, for any $a,b\in\R$, we have
	\[
		\max\{0,a+b\}\leq \max\{0,a\}+\max\{0,b\}.
	\]
	If both $a$ and $b$ are negative, then the above is an equality. If exactly one of $a$ and $b$ is negative and $a+b\leq0$ then the inequality clearly holds. If both are nonnegative then it is also an equality.
	\vskip 0.1cm
	Recall that a function $f$ is convex if for any $t\in[0,1]$ and for any $x,y$ in its domain, $f(tx+(1-t)y)\leq tf(x)+(1-t)f(y)$. We will show that $\ell((x,y),w)$ has this property with respect to $w$. Let $w_1,w_2\in\R^d$ be arbitrary and let $t\in[0,1]$. Then $0\leq(1-t)\leq1$, and so
	\begin{align*}
		\ell\left((x,y),tw_1+(1-t)w_2\right) &= \max\{0,1-y(tw_1+(1-t)w_2)\cdot x\}\\
		&= \max\{0,1-tyw_1\cdot x-(1-t)yw_2\cdot x \}\\
		&= \max\{0,1+(t-t)-tyw_1\cdot x-(1-t)yw_2\cdot x \}\\
		&= \max\{0,[t-tyw_1\cdot x] + [(1-t)-(1-t)yw_2\cdot x] \}\\
		&\leq \max\{0,t-tyw_1\cdot x\}+\max\{(1-t)-(1-t)`yw_2\cdot x \}\\
		&= t\max\{0,1-yw_1\cdot x\}+(1-t)\max\{1-yw_2\cdot x \}\\
		&= t\ell((x,y),w_1)+(1-t)\ell((x,y),w_2).
	\end{align*}
	Therefore $\ell((x,y),w)$ is convex with respect to $w$.
	\item By its definition it is clear that $0\leq\ell((x,y),w)$. If $y_i=\mathrm{sgn}(w\cdot x_i)$ then $y_iw\cdot x_i\geq 0$, implying that $1- y_iw\cdot x_i\leq 1$. Hence $\ell((x_i,y_i),w)=\max\{0,1-y_iw\cdot x_i\}\leq 1$. Combining these bounds we get
	\[
		0\leq \ell((x_i,y_i),w)\leq 1
	\]
	for correctly classified points.
	\item Observe that if we misclassify a point (so that $y_i=-\mathrm{sgn}(w\cdot x_i)$) then $y_iw\cdot x_i\geq0$. Hence $\ell((x_i,y_i),w)\geq 1$ for misclassified points. Let $I\subset \{1,2,\dots,n\}$ be the indices corresponding to the data points which $w$ misclassifies. It follows that $|I|=M(w)$. By the previous part we know that for correct classifications the hinge loss is bounded between 0 and 1. Putting this all together we obtain
	\[
		M(w) = \sum_{i=1}^{M(w)}1 = \sum_{i\in I}1 \leq \sum_{i\in I}\ell((x_i,y_i),w)\leq\sum^n_{i=1}\ell((x_i,y_i),w)= \sum^n_{i=1}\max\{0,1-y_iw\cdot x_i\}.
	\]
	Dividing both sides by $n$ gives the desired result.
\end{enumerate}



\end{document}

