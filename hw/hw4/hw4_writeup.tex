\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\usepackage{subfig}
\usepackage{enumerate}          % For enumerates indexed by letters
\usepackage{bm}                 % For bold letters
\usepackage{algorithm2e}        % For pseudocode
% \usepackage{hyperref}                % For urls
% \usepackage{bbold}              % For indicator function


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass:\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\setcounter{section}{-1}




%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework 4}
\newcommand{\hmwkDueDate}{December 12, 2016}
\newcommand{\hmwkClass}{CSE 546}
\newcommand{\hmwkAuthorName}{Brian de Silva}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ }\\
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}


% Useful commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\Z}{\mathbb{Z}}
\DeclareMathOperator*{\argmax}{arg\,max}


\begin{document}

\maketitle

\pagebreak

% Problem 0
\section{Collaborators and Acknowledgements}
% I read the post at \url{http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/} when working problem two.

I collaborated with the following people
\begin{itemize}
    \item Kathleen Champion: problems two and three
    \item Scott Moe: problem three
\end{itemize}

% Problem 1
\section{Manual Calculation of one round of EM for a GMM}
Let $x=\bbm 1&10&20\ebm$ be the set of 1-D data points we are given.

\subsection{M step}
Suppose the output from the E step is the following matrix
\[
    R = \bbm 1 & 0 \\ 0.4 &0.6 \\ 0 & 1 \ebm
\]
where entry $R_{i,c}$ is the probability of observation $x_i$ belonging to cluster $c$.
\begin{enumerate}
    \item We are trying to maximize
    \begin{align*}
        \mathbb{E}[\log Pr(\mathbb{X},\Z|\theta)] &=\mathbb{E}_{\Z\sim Pr(\Z|\X,\theta^{(t-1)})}\left[\log Pr(\X,\Z|\theta) \right]\\
        &=\mathbb{E}_{\Z\sim Pr(\Z|\X,\theta^{(t-1)})}\left[\log \sum_i\left(Pr(x_i,z_i|\theta) \right) \right]\\
        &=\sum_i\mathbb{E}_{\Z\sim Pr(\Z|\X,\theta^{(t-1)})}\left[ \log Pr(x_i,z_i|\theta)\right]\\
        &=\sum_i\mathbb{E}_{\Z\sim Pr(\Z|\X,\theta^{(t-1)})}\left[\log\left[\prod_{c} \pi_cPr(x_i|\theta_c))^{\mathbb{I}(z_i=c)}\right]\right] \\ 
        &= \sum_i\sum_{c}\mathbb{E}_{\Z\sim Pr(\Z|\X,\theta^{(t-1)})}[\mathbb{I}(z_i=c)]\log(\pi_c Pr(x_i,\theta_c))\\
        &=\sum_i\sum_cPr(z_i=c|x_i,\theta^{(t-1)})\log(\pi_cPr(x_i|\theta_c))\\
        &= \sum_i\sum_cR_{ic}(\log\pi_c + \log Pr(x_i|\theta_c))
    \end{align*}
    over the parameters associated with each class, collected in $\theta_c$. 
    \item Let $R_k = \sum_i R_{i,k}$. Then for $k=1,2$
    \[
        \pi_k = \frac13\sum^3_{i=1}R_{i,k} = R_k / 3.
    \]
    Hence
    \begin{align*}
        \pi_1 &= \frac13(1+0.4)=\frac{7}{15},\\
        \pi_2 &= \frac13(0+1) = \frac{8}{15}.
    \end{align*}
    \item Recall that for $k=1,2$ the means are given by
    \[
        \mu_k = \frac{\sum_{i=1}^3R_{i,k}x_i}{R_k}
    \]
    with $R_1=\tfrac{7}{5}$ and $R_2=\tfrac{8}{5}$. Hence
    \begin{align*}
        \mu_1 &= \frac{1\cdot 1+0.4\cdot10}{7/5} = \frac{25}{7},\\
        \mu_2 &= \frac{0.6\cdot10+1\cdot20}{8/5} = \frac{65}{4}.
    \end{align*}
    \item Recall that for $k=1,2$, the standard deviations $\sigma_k$ are
    \[
        \sigma_k=\frac{\sum_{i=1}^3R_{i,k}x_i^2}{R_k} - \mu_k^2.
    \]
    Therefore
    \begin{align*}
        \sigma_1 &= \sqrt{\frac{1\cdot 1 + 0.4\cdot100}{7/5} -\left(\frac{25}{7}\right)^2} = \sqrt{\frac{810}{49}},\\
        \sigma_2 &= \sqrt{\frac{0.6\cdot 100+1\cdot400}{8/5}-\left(\frac{65}{4}\right)^2} = \sqrt{\frac{375}{16}}.
    \end{align*}
\end{enumerate}

\subsection{E step}
\begin{enumerate}
    \item 
    \[
        Pr(z_i=c)=R_{i,c} = \frac{\pi^{(t-1)}_cPr\left(x_i\left|\theta^{(t-1)}_c\right.\right)}{\sum^2_{c'=1}\pi_{c'}Pr\left(x_i\left|\theta_{c'}^{(t-1)}\right.\right)},
    \]
    where
    \[
        Pr\left(x_i\left|\theta^{(t-1)}_c\right.\right) = \frac{\pi_c^{(t-1)}}{\sqrt{2\pi}\sigma_c^{(t-1)}}exp\left(-\left(x_i-\mu_c^{(t-1)}\right)^2\left/2\left(\sigma^{(t-1)}_c\right)^2\right.\right).
    \]
    \item Substituting in the values $\theta^{(t-1)}$ we obtained in the M step into the above formula, we obtain
    \begin{align*}
        Pr\left(1\left|\theta^{(t-1)}_1\right.\right)&\approx 0.0374898 \\
        Pr\left(1\left|\theta^{(t-1)}_2\right.\right)&\approx 0.000307803\\
        Pr\left(1\left|\theta^{(t-1)}_1\right.\right)+Pr\left(1\left|\theta^{(t-1)}_2\right.\right)&\approx  0.0377976\\
        &\implies R_{1,1}\approx 0.991857,~~R_{1,2}\approx 0.00814346.
    \end{align*}
    \begin{align*}
        Pr\left(10\left|\theta^{(t-1)}_1\right.\right)&\approx 0.0131191 \\
        Pr\left(10\left|\theta^{(t-1)}_2\right.\right)&\approx  0.0191003\\
        Pr\left(10\left|\theta^{(t-1)}_1\right.\right)+Pr\left(10\left|\theta^{(t-1)}_2\right.\right)&\approx  0.0322194\\
        &\implies R_{2,1}\approx 0.40718,~~R_{2,2}\approx 0.59282.
    \end{align*}
    \begin{align*}
        Pr\left(20\left|\theta^{(t-1)}_1\right.\right)&\approx  0.0000130429\\
        Pr\left(20\left|\theta^{(t-1)}_2\right.\right)&\approx 0.0325585 \\
        Pr\left(20\left|\theta^{(t-1)}_1\right.\right)+Pr\left(20\left|\theta^{(t-1)}_2\right.\right)&\approx 0.0325716\\
        &\implies R_{3,1}\approx 0.000400438 ,~~R_{3,2}\approx 0.9996.
    \end{align*}
    Putting this all together, the new value of $R$ is 
    \[
        R=\bbm 0.991857 & 0.00814346 \\
        0.40718 & 0.59282 \\
        0.000400438 & 0.9996
        \ebm.
    \]
\end{enumerate}

% Problem 2
\section{Neural Nets and Backprop}
\subsection{With tanh hidden units}
The code for this problem is in \texttt{hw4-2-1.py}.
\begin{enumerate}
    \item We used the following parameter choices:
    \begin{itemize}
        \item Learning rate: I started with an initial learning rate of $10^{-3}$ which I reduced by a factor of $4$ every 10 epochs.
        \item Mini-batch size: 10.
        \item Weight initalization: I initialized the weights in steps. Let $W_i$ and $W_h$ denote the weights for the input layer and hidden layer, respectively. Then my initialization procedure was as follows
        \begin{enumerate}
            \item Set each entry of $W_i$ by sampling from a distribution $N(0,1/\mathbb{E}[\|X\|^2])$,where $X$ is the training data matrix.
            \item Set each entry of $W_h$ by sampling from a distribution $N(0,1/\sqrt{n})$, where $n$ is the number of input nodes (50 in this case).
            \item Perform one forward pass to obtain the model's predictions $\hat Y$ using the above weights.
            \item Set 
            \begin{align*}
            W_i &\leftarrow W_i / (100 \mathbb{E}[\hat Y]),\\
            W_h &\leftarrow W_h / (100 \mathbb{E}[\hat Y])
            \end{align*}
            to attempt to ensure that the expectation of the subsequent output is less than 0.1 times the expecation of the training labels, $Y$. In this case $Y$ consists of one-hot vectors of length 10 encoding the digit. Hence $\mathbb{E}[Y]=0.1$, which is why there is a factor of $1/100$ in the above.
        \end{enumerate}
        \item The method was run for 30 epochs.
        \item Other: No regularizations or offsets were used.
        
    \end{itemize}
    \item
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{sqLoss_2-1-30Ep}
        \caption{The square loss on the training and test sets for a 2-layer neural network with $\tanh$ activation functions} 
        \label{fig:sqLoss_2-1}
    \end{figure}
    Figure \ref{fig:sqLoss_2-1} shows the decay of the mean square error as a function of epochs. The error is plotted at every half-epoch.
    \item
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{z1Loss_2-1-30Ep}
        \caption{The 0/1 loss on the training and test sets for a 2-layer neural network with $\tanh$ activation functions} 
        \label{fig:z1Loss_2-1}
    \end{figure}
    Figure \ref{fig:z1Loss_2-1} shows the decay of the 0/1 loss as a function of epochs. The error is plotted at every half-epoch.
    \item My final losses are summarized in the following table

    \begin{tabular}{l|ll}
    & Square loss & 0/1 loss \\
    \hline
    Training &  0.05566 & 0.0142 \\
    Testing  &  0.074909 & 0.0249
    \end{tabular}
    \item 
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{weightVisualization_2-1_30Ep}
        \caption{Visualizations of learned weights for 10 random nodes with $\tanh$ activation functions} 
        \label{fig:weights2-1}
    \end{figure}
    Figure \ref{fig:weights2-1} provides grayscale visualizations of the weights of 10 random nodes from the hidden layer of our network.
\end{enumerate}

\subsection{With ReLu hidden units}
The code for this problem is in \texttt{hw4-2-2.py}.
\begin{enumerate}
    \item We used the exact same parameter configuration and initialization procedure for this problem as in the previous one.
    \item
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{sqLoss_2-2-30Ep}
        \caption{The square loss on the training and test sets for a 2-layer neural network with $\tanh$ activation functions} 
        \label{fig:sqLoss_2-2}
    \end{figure}
    Figure \ref{fig:sqLoss_2-2} shows the decay of the mean square error as a function of epochs. The error is plotted at every half-epoch.
    \item
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{z1Loss_2-2-30Ep}
        \caption{The 0/1 loss on the training and test sets for a 2-layer neural network with $\tanh$ activation functions in the hidden layer} 
        \label{fig:z1Loss_2-2}
    \end{figure}
    Figure \ref{fig:z1Loss_2-2} shows the decay of the 0/1 loss as a function of epochs. The error is plotted at every half-epoch.
    \item My final losses are summarized in the following table

    \begin{tabular}{l|ll}
    & Square loss & 0/1 loss \\
    \hline
    Training & 0.055168  & 0.009900 \\
    Testing  & 0.071822  & 0.017300
    \end{tabular}

    \item 
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{weightVisualization_2-2_30Ep}
        \caption{Visualizations of learned weights for 10 random nodes with ReLu activation functions in the hidden layer} 
        \label{fig:weights2-2}
    \end{figure}
    Figure \ref{fig:weights2-2} gives a grayscale visualizations of the weights of 10 random nodes from the hidden layer of our network. These weights were taken from the same network as the previous images in this section.
\end{enumerate}

% Train Square loss after 30 epochs:   0.055168
% Train 0/1 loss after 30 epochs:      0.009900
% Test Square loss after 30 epochs:    0.071822
% Test 0/1 loss after 30 epochs:       0.017300

\subsection{With ReLu hidden units and ReLu output units}
The code for this problem is in \texttt{hw4-2-3.py}.

    \begin{enumerate}
    \item We used the exact same parameter configuration and initialization procedure for this problem as in the previous one.
    \item
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{sqLoss_2-3-30Ep}
        \caption{The square loss on the training and test sets for a 2-layer neural network with ReLu activation functions in the hidden and output layers} 
        \label{fig:sqLoss_2-3}
    \end{figure}
    Figure \ref{fig:sqLoss_2-3} shows the decay of the mean square error as a function of epochs. The error is plotted at every half-epoch.
    \item
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{z1Loss_2-3-30Ep}
        \caption{The 0/1 loss on the training and test sets for a 2-layer neural network with ReLu activation functions in the hidden and output layers} 
        \label{fig:z1Loss_2-3}
    \end{figure}
    Figure \ref{fig:z1Loss_2-3} shows the decay of the 0/1 loss as a function of epochs. The error is plotted at every half-epoch.
    \item My final losses are summarized in the following table

    \begin{tabular}{l|ll}
    & Square loss & 0/1 loss \\
    \hline
    Training & 0.018621  & 0.003817 \\
    Testing  & 0.041433  & 0.014300
    \end{tabular}

    \item 
    \begin{figure}
        \centering
        \includegraphics[width=.85\textwidth]{weightVisualization_2-3_30Ep}
        \caption{Visualizations of learned weights for 10 random nodes with ReLu activation functions in the latter two layers} 
        \label{fig:weights2-3}
    \end{figure}
    Figure \ref{fig:weights2-3} gives a grayscale visualizations of the weights of 10 random nodes from the hidden layer of our network. These weights were taken from the same network as the previous images in this section.
\end{enumerate}

    % \item Params: 30 epochs, batch size 10, step size 1.e-3, no reg or offset, cut down weights by a factor of 4 every 10 epochs, same weight normalization in all cases except no renormalization of input weights
    % \item sq train:    0.018621 
    % \item 0/1 train:   0.003817
    % \item sq test:     0.041433 
    % \item 0/1 test:    0.014300

% Problem 3
\section{EM vs. Gradient Descent}
Let $L(\theta) = \log Pr(\X,\theta)$.
\begin{enumerate}
    \item The left-hand side can be expressed as
    \[
        \nabla L(\theta) = \nabla \log Pr(\X|\theta) = \frac{\nabla Pr(\X|\theta)}{Pr(\X|\theta)}.
    \]
    Note that $Pr(\X,\Z|\theta) = Pr(\Z|\X,\theta)Pr(\X|\theta)$. This allows us to write the right-hand-side as
    \begin{align*}
        \mathbb{E}_{\Z\sim Pr(\Z|\X,\theta)}\left[\nabla\log Pr(\X,\Z|\theta)\right] &= \mathbb{E}_{\Z\sim Pr(\Z|\X,\theta)}\left[\frac{\nabla Pr(\X,\Z|\theta)}{Pr(\X,\Z|\theta)}\right]\\
        &= \mathbb{E}_{\Z\sim Pr(\Z|\X,\theta)}\left[\frac{\nabla Pr(\X,\Z|\theta)}{Pr(\Z|\X,\theta)P(\X|\theta)}\right]\\
        &=\sum_Z Pr(Z|\X,\theta)\frac{\nabla Pr(\X,Z|\theta)}{Pr(Z|\X,\theta)Pr(\X|\theta)}\\
        &=\frac{\sum_Z\nabla Pr(\X,Z|\theta)}{Pr(\X|\theta)}\\
        &= \frac{\nabla Pr(\X|\theta)}{Pr(\X|\theta)}.
    \end{align*}
    From here it is easy to see that the two are equal.
    \item Let $\theta^t$ denote the parameters Alice and Bob start with. Alice's update will be of the form
    \[
        \theta^{t+1} \leftarrow \theta^t + \eta \nabla L(\theta^t).
    \]
    During the E step Bob will use $\theta^t$ to compute $Pr(z_i=k|x_i\theta^t)$. Thus, using our previous result, we see that Bob's update step will be
    \begin{align*}
        \theta^{t+1} &= \theta^t + \eta \nabla\left.\mathbb{E}_{\Z\sim Pr(\Z|\X,\theta^t)}\left[\log Pr(\X,\Z|\theta)\right]\right|_{\theta=\theta^t}\\
        &=\theta^t+\eta \nabla\left.\sum_ZPr(Z|\X,\theta^t)\log Pr(Z|\X,\theta)\right|_{\theta=\theta^t}\\
        &=\theta^t+\eta\left.\sum_ZPr(Z|\X,\theta^t)\nabla\log Pr(Z|\X,\theta)\right|_{\theta=\theta^t}\\
        &=\theta^t+\eta\left.\mathbb{E}_{\Z\sim Pr(\Z|\X,\theta^t)}\left[\nabla \log Pr(\X,\Z|\theta)\right]\right|_{\theta=\theta^t}\\
        &=\theta^t + \eta \nabla L(\theta^t),
    \end{align*}
    which is the same as Alice's.

    \item If you run the EM algorithm to convergence then you \textbf{do} reach a critical point of the likelihood function. To see this suppose that the EM algorithm converges to some $\hat \theta$, but this is not a critical point of the likelihood function, i.e. $\nabla L(\hat\theta)\neq0$. Since EM has converged, applying the M step to $\hat \theta$ just gives back $\hat\theta$:
    \[
        \hat \theta = \argmax_\theta \mathbb{E}_{\Z\sim Pr(\Z|\X,\hat\theta)}[\log Pr(\X,\Z|\theta)].
    \]
    But if $\nabla L(\hat\theta)\neq0$, then we can use a gradient step (with a small enough learning rate $\eta$) to find a $\tilde\theta$ that beats $\hat \theta$:
    \begin{align*}
        \tilde\theta &= \hat\theta + \eta\nabla (\left.\mathbb{E}_{\Z\sim Pr(\Z|\X,\hat\theta)}[\log Pr(\X,\Z|\theta)])\right|_{\theta=\hat\theta}\\
        &=\hat\theta + \eta\nabla L(\hat\theta).
    \end{align*}
    This contradicts the way $\hat\theta$ is chosen (as an argmax), so we must have the $\nabla L(\hat\theta)=0$.
\end{enumerate}


% Problem 4
\section{Markov Decision Processes and Dynamic Programming}
\begin{enumerate}
    \item Note that $\sum_{x'}Pr(x'|x,a) = 1$.
    For any two value functions $V_1,~V_2$, we have
    \begin{align*}
        \|Bell(V_1)-Bell(V_2)\|_{\infty} &= \max_s\left|\tilde V_1(s)-\tilde V_2(s)\right|\\
        &= \max_s\left|\max_a\left(R(s,a) +\gamma\sum_{x'}Pr(x'|s,a)V_1(x')\right)\right.\\&\quad\left.-\max_b\left(R(s,b) +\gamma\sum_{x'}Pr(x'|s,b)V_2(x')\right) \right| \\
        &\leq \max_s\left| \max_a\left(\gamma\sum_{x'}Pr(x'|s,a)V_1(x')-\gamma\sum_{x'}Pr(x'|s,b)V_2(x')\right)\right|\\
        &= \max_s\left|\gamma\max_a\left(\sum_{x'}Pr(x'|s,a)\left(V_1(x')-V_2(x')\right)\right)\right|\\
        &\leq \max_s\left|\gamma\max_a\left( \sum_{x'}Pr(x'|s,a)\max_{x}(V_1(x)-V_2(x))\right) \right|\\
        &= \gamma \max_s\left|\max_x(V_1(x)-V_2(x)) \max_a\left(\sum_{x'}Pr(x'|s,a)\right) \right|\\
        &= \gamma \left|\max_x(V_1(x)-V_2(x)) \right| \\
        &=\gamma \max_x\left|V_1(x)-V_2(x)\right|\\
        &= \gamma \|V_1-V_2\|_{\infty}.
    \end{align*}
    Thus the Bellman operator is a contraction mapping.
    \item Let $V$ be a fixed point of the Bellman operator, so that $Bell(V)=V$. Suppose $U$ is also a fixed point of the Bellman operator. Then by the above
    \[
        \|U-V\|_{\infty} = \|Bell(U)-Bell(V)\|_{\infty}\leq \gamma \|U-V\|_{\infty}.
    \]
    This implies that $(1-\gamma)\|U-V\|_{\infty}\leq0$. But $1-\gamma>0$ by assumption, so the only way that this can be true is if $\|U-V\|_{\infty}=0$. So we must have
    \[
        \|U-V\|_{\infty}=\max_s\|U(s)-V(s)\|=0,
    \]
    from which it follows that $U=V$. Hence $V$ must be unique.
    \item If we find a $V$ such that $Bell(V)=V$, then $V$ is the optimal value function and we can determine the optimal policy $\Pi(x)$ which specifies the action to be taken in state $x$. First we evaluate $V(x)$. Then we know that the optimal action to be taken, $a$ maximizes
    \[
        \max_a\left(R(s,a) +\gamma\sum_{x'}Pr(x'|s,a)V(x')\right),
    \]
    where $V$ is \textit{known}. Hence we can just select this $a$ as our action, i.e.
    \[
        \Pi(x) = \argmax_a\left(R(s,a) +\gamma\sum_{x'}Pr(x'|s,a)V(x')\right)
    \]
\end{enumerate}

\end{document}

